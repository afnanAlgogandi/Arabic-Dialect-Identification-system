{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Arabic Dialect Identification system\n","\n","**CCAI-413: Natural Language Processing project.**\n","\n","\n","------------------------------------------------------------------\n","\n","# About \n","Is an Arabic Dialect Identification system. its task of identifying the dialect of Arabic language in a text format. It is a challenging task due to the high variability of Arabic dialects and the lack of large-scale annotated datasets.  \n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# important Library"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T05:12:16.079960Z","iopub.status.busy":"2023-05-22T05:12:16.079611Z","iopub.status.idle":"2023-05-22T05:12:19.296723Z","shell.execute_reply":"2023-05-22T05:12:19.295634Z","shell.execute_reply.started":"2023-05-22T05:12:16.079929Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk \n","\n","nltk.download('wordnet')\n","from nltk.stem.isri import ISRIStemmer\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import GridSearchCV"]},{"cell_type":"markdown","metadata":{},"source":["# Load Dataset"]},{"cell_type":"markdown","metadata":{},"source":["**Messages Dataset:**\n","\n","Data contains tweets in different Arabic dialects."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T05:12:19.301456Z","iopub.status.busy":"2023-05-22T05:12:19.299470Z","iopub.status.idle":"2023-05-22T05:12:22.399676Z","shell.execute_reply":"2023-05-22T05:12:22.398676Z","shell.execute_reply.started":"2023-05-22T05:12:19.301426Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweets</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.175358e+18</td>\n","      <td>@Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.175416e+18</td>\n","      <td>@7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.175450e+18</td>\n","      <td>@KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.175471e+18</td>\n","      <td>@HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.175497e+18</td>\n","      <td>@hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>458656</th>\n","      <td>1.057419e+18</td>\n","      <td>@mycousinvinnyys @hanyamikhail1 Ù…ØªÙ‡ÙŠØ§Ù„ÙŠ Ø¯ÙŠ Ø´ÙƒÙˆ...</td>\n","    </tr>\n","    <tr>\n","      <th>458657</th>\n","      <td>1.055620e+18</td>\n","      <td>@MahmoudWaked7 @maganenoo ÙÙŠ Ø·Ø±ÙŠÙ‚ Ù…Ø·Ø±ÙˆØ­ Ù…Ø±ÙƒØ² Ø¨...</td>\n","    </tr>\n","    <tr>\n","      <th>458658</th>\n","      <td>NaN</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>458659</th>\n","      <td>1.057419e+18</td>\n","      <td>@mycousinvinnyys @hanyamikhail1 Ù…ØªÙ‡ÙŠØ§Ù„ÙŠ Ø¯ÙŠ Ø´ÙƒÙˆ...</td>\n","    </tr>\n","    <tr>\n","      <th>458660</th>\n","      <td>1.055620e+18</td>\n","      <td>@MahmoudWaked7 @maganenoo ÙÙŠ Ø·Ø±ÙŠÙ‚ Ù…Ø·Ø±ÙˆØ­ Ù…Ø±ÙƒØ² Ø¨...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>458661 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                  id                                             tweets\n","0       1.175358e+18   @Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .\n","1       1.175416e+18  @7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...\n","2       1.175450e+18                    @KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ\n","3       1.175471e+18         @HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’\n","4       1.175497e+18                 @hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº\n","...              ...                                                ...\n","458656  1.057419e+18  @mycousinvinnyys @hanyamikhail1 Ù…ØªÙ‡ÙŠØ§Ù„ÙŠ Ø¯ÙŠ Ø´ÙƒÙˆ...\n","458657  1.055620e+18  @MahmoudWaked7 @maganenoo ÙÙŠ Ø·Ø±ÙŠÙ‚ Ù…Ø·Ø±ÙˆØ­ Ù…Ø±ÙƒØ² Ø¨...\n","458658           NaN                                                  0\n","458659  1.057419e+18  @mycousinvinnyys @hanyamikhail1 Ù…ØªÙ‡ÙŠØ§Ù„ÙŠ Ø¯ÙŠ Ø´ÙƒÙˆ...\n","458660  1.055620e+18  @MahmoudWaked7 @maganenoo ÙÙŠ Ø·Ø±ÙŠÙ‚ Ù…Ø·Ø±ÙˆØ­ Ù…Ø±ÙƒØ² Ø¨...\n","\n","[458661 rows x 2 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Load messages dataset\n","tweets = pd.read_csv('/kaggle/input/aim-technologies-predict-the-dialectal-arabic/messages.csv',lineterminator='\\n')\n","column_names = ['id', 'tweets'] # list of column names\n","tweets.columns = column_names # Label the columns\n","tweets"]},{"cell_type":"markdown","metadata":{},"source":["**Dialect Dataset:**\n","\n","Data contains the dialect (labels) of the tweets."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T05:12:22.412332Z","iopub.status.busy":"2023-05-22T05:12:22.411718Z","iopub.status.idle":"2023-05-22T05:12:22.703647Z","shell.execute_reply":"2023-05-22T05:12:22.702614Z","shell.execute_reply.started":"2023-05-22T05:12:22.412297Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>dialect</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1175358310087892992</td>\n","      <td>IQ</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1175416117793349632</td>\n","      <td>IQ</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1175450108898565888</td>\n","      <td>IQ</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1175471073770573824</td>\n","      <td>IQ</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1175496913145217024</td>\n","      <td>IQ</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>458192</th>\n","      <td>1019484980282580992</td>\n","      <td>BH</td>\n","    </tr>\n","    <tr>\n","      <th>458193</th>\n","      <td>1021083283709407232</td>\n","      <td>BH</td>\n","    </tr>\n","    <tr>\n","      <th>458194</th>\n","      <td>1017477537889431552</td>\n","      <td>BH</td>\n","    </tr>\n","    <tr>\n","      <th>458195</th>\n","      <td>1022430374696239232</td>\n","      <td>BH</td>\n","    </tr>\n","    <tr>\n","      <th>458196</th>\n","      <td>1022409931029458944</td>\n","      <td>BH</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>458197 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                         id dialect\n","0       1175358310087892992      IQ\n","1       1175416117793349632      IQ\n","2       1175450108898565888      IQ\n","3       1175471073770573824      IQ\n","4       1175496913145217024      IQ\n","...                     ...     ...\n","458192  1019484980282580992      BH\n","458193  1021083283709407232      BH\n","458194  1017477537889431552      BH\n","458195  1022430374696239232      BH\n","458196  1022409931029458944      BH\n","\n","[458197 rows x 2 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Load dialect dataset\n","dialects = pd.read_csv(\"/kaggle/input/aim-technologies-predict-the-dialectal-arabic/dialect_dataset.csv\")\n","dialects"]},{"cell_type":"markdown","metadata":{},"source":["# Data processing"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T05:12:22.709619Z","iopub.status.busy":"2023-05-22T05:12:22.705015Z","iopub.status.idle":"2023-05-22T05:12:23.265382Z","shell.execute_reply":"2023-05-22T05:12:23.264460Z","shell.execute_reply.started":"2023-05-22T05:12:22.709574Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweets</th>\n","      <th>dialect</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>@Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .</td>\n","      <td>IQ</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>@7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...</td>\n","      <td>IQ</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ</td>\n","      <td>IQ</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’</td>\n","      <td>IQ</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>@hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº</td>\n","      <td>IQ</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>458196</th>\n","      <td>@Al_mhbaa_7 Ù…Ø¨Ø³ÙˆØ·ÙŠÙ† Ù…Ù†Ùƒ Ø§Ù„Ù„ÙŠ Ø¨Ø§Ø³Ø·Ø§Ù†Ø§ğŸ˜…</td>\n","      <td>BH</td>\n","    </tr>\n","    <tr>\n","      <th>458197</th>\n","      <td>@Zzainabali @P_ameerah ÙˆØ§Ù„Ù„Ù‡ Ù…Ø§ÙŠÙ†Ø¯Ù‡ Ø§Ø¨Ø´ ÙŠØ®ØªÙŠ</td>\n","      <td>BH</td>\n","    </tr>\n","    <tr>\n","      <th>458198</th>\n","      <td>@Al_mhbaa_7 Ø´Ùˆ Ø¹Ù…Ù„Ù†Ø§ Ù„Ùƒ Ø­Ù†Ø§ ØªÙ‡Ø±Ø¨ÙŠ Ù…Ù†Ù†Ø§ Ø§Ø­Ù†Ø§ Ù…Ø³...</td>\n","      <td>BH</td>\n","    </tr>\n","    <tr>\n","      <th>458199</th>\n","      <td>@haneenalmwla Ø§Ù„Ù„Ù‡ ÙŠØ¨Ø§Ø±Ùƒ ÙÙŠÙ‡Ø§ ÙˆØ¨Ø§Ù„Ø¹Ø§ÙÙŠÙ‡ ğŸ˜‹ğŸ˜‹ğŸ˜‹</td>\n","      <td>BH</td>\n","    </tr>\n","    <tr>\n","      <th>458200</th>\n","      <td>@jolnar121 Ø§Ù„Ø³Ø­Ù„Ù‡ Ø¶ÙŠÙÙŠ ÙŠ Ø¨ØªØ·Ù„Ø¹ Ù„Ùƒ Ø³Ø­Ù„ÙŠÙ‡ğŸ˜…ğŸ˜…</td>\n","      <td>BH</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>458201 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                                                   tweets dialect\n","0        @Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .      IQ\n","1       @7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...      IQ\n","2                         @KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ      IQ\n","3              @HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’      IQ\n","4                      @hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº      IQ\n","...                                                   ...     ...\n","458196              @Al_mhbaa_7 Ù…Ø¨Ø³ÙˆØ·ÙŠÙ† Ù…Ù†Ùƒ Ø§Ù„Ù„ÙŠ Ø¨Ø§Ø³Ø·Ø§Ù†Ø§ğŸ˜…      BH\n","458197       @Zzainabali @P_ameerah ÙˆØ§Ù„Ù„Ù‡ Ù…Ø§ÙŠÙ†Ø¯Ù‡ Ø§Ø¨Ø´ ÙŠØ®ØªÙŠ      BH\n","458198  @Al_mhbaa_7 Ø´Ùˆ Ø¹Ù…Ù„Ù†Ø§ Ù„Ùƒ Ø­Ù†Ø§ ØªÙ‡Ø±Ø¨ÙŠ Ù…Ù†Ù†Ø§ Ø§Ø­Ù†Ø§ Ù…Ø³...      BH\n","458199        @haneenalmwla Ø§Ù„Ù„Ù‡ ÙŠØ¨Ø§Ø±Ùƒ ÙÙŠÙ‡Ø§ ÙˆØ¨Ø§Ù„Ø¹Ø§ÙÙŠÙ‡ ğŸ˜‹ğŸ˜‹ğŸ˜‹      BH\n","458200          @jolnar121 Ø§Ù„Ø³Ø­Ù„Ù‡ Ø¶ÙŠÙÙŠ ÙŠ Ø¨ØªØ·Ù„Ø¹ Ù„Ùƒ Ø³Ø­Ù„ÙŠÙ‡ğŸ˜…ğŸ˜…      BH\n","\n","[458201 rows x 2 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Marge tweets and dialects datasets in one dataframe\n","data = pd.merge(tweets, dialects, on='id')\n","\n","# drop the id columns\n","data = data.drop(columns=['id'])\n","\n","data"]},{"cell_type":"markdown","metadata":{},"source":["**Dialect names**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T05:12:23.268903Z","iopub.status.busy":"2023-05-22T05:12:23.266993Z","iopub.status.idle":"2023-05-22T05:12:23.307458Z","shell.execute_reply":"2023-05-22T05:12:23.306304Z","shell.execute_reply.started":"2023-05-22T05:12:23.268861Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['IQ' 'LY' 'QA' 'PL' 'SY' 'TN' 'JO' 'MA' 'SA' 'YE' 'DZ' 'EG' 'LB' 'KW'\n"," 'OM' 'SD' 'AE' 'BH']\n"]}],"source":["# Display the dialect names\n","dialect_names = data['dialect'].unique()\n","print(dialect_names)"]},{"cell_type":"markdown","metadata":{},"source":["**Convert dialectal Arabic names to full country names**"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T05:12:23.309686Z","iopub.status.busy":"2023-05-22T05:12:23.309276Z","iopub.status.idle":"2023-05-22T05:12:23.438711Z","shell.execute_reply":"2023-05-22T05:12:23.437570Z","shell.execute_reply.started":"2023-05-22T05:12:23.309651Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweets</th>\n","      <th>dialect</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>@Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .</td>\n","      <td>Ø¹Ø±Ø§Ù‚ÙŠ</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>@7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...</td>\n","      <td>Ø¹Ø±Ø§Ù‚ÙŠ</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ</td>\n","      <td>Ø¹Ø±Ø§Ù‚ÙŠ</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’</td>\n","      <td>Ø¹Ø±Ø§Ù‚ÙŠ</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>@hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº</td>\n","      <td>Ø¹Ø±Ø§Ù‚ÙŠ</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>458196</th>\n","      <td>@Al_mhbaa_7 Ù…Ø¨Ø³ÙˆØ·ÙŠÙ† Ù…Ù†Ùƒ Ø§Ù„Ù„ÙŠ Ø¨Ø§Ø³Ø·Ø§Ù†Ø§ğŸ˜…</td>\n","      <td>Ø¨Ø­Ø±ÙŠÙ†ÙŠ</td>\n","    </tr>\n","    <tr>\n","      <th>458197</th>\n","      <td>@Zzainabali @P_ameerah ÙˆØ§Ù„Ù„Ù‡ Ù…Ø§ÙŠÙ†Ø¯Ù‡ Ø§Ø¨Ø´ ÙŠØ®ØªÙŠ</td>\n","      <td>Ø¨Ø­Ø±ÙŠÙ†ÙŠ</td>\n","    </tr>\n","    <tr>\n","      <th>458198</th>\n","      <td>@Al_mhbaa_7 Ø´Ùˆ Ø¹Ù…Ù„Ù†Ø§ Ù„Ùƒ Ø­Ù†Ø§ ØªÙ‡Ø±Ø¨ÙŠ Ù…Ù†Ù†Ø§ Ø§Ø­Ù†Ø§ Ù…Ø³...</td>\n","      <td>Ø¨Ø­Ø±ÙŠÙ†ÙŠ</td>\n","    </tr>\n","    <tr>\n","      <th>458199</th>\n","      <td>@haneenalmwla Ø§Ù„Ù„Ù‡ ÙŠØ¨Ø§Ø±Ùƒ ÙÙŠÙ‡Ø§ ÙˆØ¨Ø§Ù„Ø¹Ø§ÙÙŠÙ‡ ğŸ˜‹ğŸ˜‹ğŸ˜‹</td>\n","      <td>Ø¨Ø­Ø±ÙŠÙ†ÙŠ</td>\n","    </tr>\n","    <tr>\n","      <th>458200</th>\n","      <td>@jolnar121 Ø§Ù„Ø³Ø­Ù„Ù‡ Ø¶ÙŠÙÙŠ ÙŠ Ø¨ØªØ·Ù„Ø¹ Ù„Ùƒ Ø³Ø­Ù„ÙŠÙ‡ğŸ˜…ğŸ˜…</td>\n","      <td>Ø¨Ø­Ø±ÙŠÙ†ÙŠ</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>458201 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                                                   tweets dialect\n","0        @Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .   Ø¹Ø±Ø§Ù‚ÙŠ\n","1       @7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...   Ø¹Ø±Ø§Ù‚ÙŠ\n","2                         @KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ   Ø¹Ø±Ø§Ù‚ÙŠ\n","3              @HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’   Ø¹Ø±Ø§Ù‚ÙŠ\n","4                      @hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº   Ø¹Ø±Ø§Ù‚ÙŠ\n","...                                                   ...     ...\n","458196              @Al_mhbaa_7 Ù…Ø¨Ø³ÙˆØ·ÙŠÙ† Ù…Ù†Ùƒ Ø§Ù„Ù„ÙŠ Ø¨Ø§Ø³Ø·Ø§Ù†Ø§ğŸ˜…  Ø¨Ø­Ø±ÙŠÙ†ÙŠ\n","458197       @Zzainabali @P_ameerah ÙˆØ§Ù„Ù„Ù‡ Ù…Ø§ÙŠÙ†Ø¯Ù‡ Ø§Ø¨Ø´ ÙŠØ®ØªÙŠ  Ø¨Ø­Ø±ÙŠÙ†ÙŠ\n","458198  @Al_mhbaa_7 Ø´Ùˆ Ø¹Ù…Ù„Ù†Ø§ Ù„Ùƒ Ø­Ù†Ø§ ØªÙ‡Ø±Ø¨ÙŠ Ù…Ù†Ù†Ø§ Ø§Ø­Ù†Ø§ Ù…Ø³...  Ø¨Ø­Ø±ÙŠÙ†ÙŠ\n","458199        @haneenalmwla Ø§Ù„Ù„Ù‡ ÙŠØ¨Ø§Ø±Ùƒ ÙÙŠÙ‡Ø§ ÙˆØ¨Ø§Ù„Ø¹Ø§ÙÙŠÙ‡ ğŸ˜‹ğŸ˜‹ğŸ˜‹  Ø¨Ø­Ø±ÙŠÙ†ÙŠ\n","458200          @jolnar121 Ø§Ù„Ø³Ø­Ù„Ù‡ Ø¶ÙŠÙÙŠ ÙŠ Ø¨ØªØ·Ù„Ø¹ Ù„Ùƒ Ø³Ø­Ù„ÙŠÙ‡ğŸ˜…ğŸ˜…  Ø¨Ø­Ø±ÙŠÙ†ÙŠ\n","\n","[458201 rows x 2 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Define a dictionary that maps dialectal Arabic names to full country names\n","short_to_full = {\n","    'EG': 'Ù…ØµØ±ÙŠ',\n","    'DZ': 'Ø¬Ø²Ø§Ø¦Ø±ÙŠ',\n","    'TN': 'ØªÙˆÙ†Ø³ÙŠ',\n","    'LY': 'Ù„ÙŠØ¨ÙŠ',\n","    'MA': 'Ù…ØºØ±Ø¨ÙŠ',\n","    'JO': 'Ø§Ø±Ø¯Ù†ÙŠ',\n","    'LB': 'Ù„Ø¨Ù†Ø§Ù†ÙŠ',\n","    'PL': 'ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ',\n","    'SY': 'Ø³ÙˆØ±ÙŠ',\n","    'IQ': 'Ø¹Ø±Ø§Ù‚ÙŠ',\n","    'KW': 'ÙƒÙˆÙŠØªÙŠ',\n","    'SA': 'Ø³Ø¹ÙˆØ¯ÙŠ',\n","    'AE': 'Ø§Ù…Ø§Ø±Ø§ØªÙŠ',\n","    'OM': 'Ø¹Ù…Ø§Ù†ÙŠ',\n","    'QA': 'Ù‚Ø·Ø±ÙŠ',\n","    'YE': 'ÙŠÙ…Ù†ÙŠ',\n","    'SD': 'Ø³ÙˆØ¯Ø§Ù†ÙŠ',\n","    'BH': 'Ø¨Ø­Ø±ÙŠÙ†ÙŠ'\n","}\n","# Define a function that converts the short names to full names\n","def convert_name(name):\n","    return short_to_full[name]\n","\n","# Convert dialectal Arabic names to full country names\n","data['dialect'] = data['dialect'].apply(convert_name)\n","\n","data"]},{"cell_type":"markdown","metadata":{},"source":["# preprocess Text\n","\n","**Filter data from some Unwanted additions, such as :**\n","> symbols (such as @ # .. etc)\n","\n","> stopwords ( such as ÙÙŠ , Ø§Ù„ Ø§Ù„ØªØ¹Ø±ÙŠÙ )\n","\n","> Reducing a word to its stem using stemming"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T05:12:23.441041Z","iopub.status.busy":"2023-05-22T05:12:23.440004Z","iopub.status.idle":"2023-05-22T05:12:23.449097Z","shell.execute_reply":"2023-05-22T05:12:23.447800Z","shell.execute_reply.started":"2023-05-22T05:12:23.441004Z"},"trusted":true},"outputs":[],"source":["def preprocessText(text):\n","    # Remove URLs, mentions, and hashtags\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n","    text = re.sub(r'@\\w+|\\#\\w+', '', text)\n","\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('arabic'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # Perform stemming\n","    stemmer = ISRIStemmer()\n","    tokens = [stemmer.stem(word) for word in tokens]\n","\n","    # Join the tokens back into a string\n","    preprocessed_text = ' '.join(tokens)\n","\n","    return preprocessed_text"]},{"cell_type":"markdown","metadata":{},"source":["# splt the data"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T05:12:23.451470Z","iopub.status.busy":"2023-05-22T05:12:23.451110Z","iopub.status.idle":"2023-05-22T05:12:23.585246Z","shell.execute_reply":"2023-05-22T05:12:23.584296Z","shell.execute_reply.started":"2023-05-22T05:12:23.451435Z"},"trusted":true},"outputs":[],"source":["# list of column names\n","column_names = ['tweets','dialect'] \n","\n","# Train data\n","train_data = data.sample(frac = 0.75) # Take 75% of the data randomly\n","train_data.columns = column_names # Label the columns\n","x_train = train_data.tweets # x = the tweets in train data\n","y_train = train_data.dialect # y = the labels of the train data\n","\n","# Test data\n","test_data = data.drop(train_data.index) # Take the remaining 25% of the data\n","test_data.columns = column_names # Label the columns\n","x_test = test_data.tweets # x = the tweets in test data\n","y_test = test_data.dialect # y = the labels of the test data"]},{"cell_type":"markdown","metadata":{},"source":["# Traning"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T05:12:23.589133Z","iopub.status.busy":"2023-05-22T05:12:23.588755Z","iopub.status.idle":"2023-05-22T05:43:13.525175Z","shell.execute_reply":"2023-05-22T05:43:13.524059Z","shell.execute_reply.started":"2023-05-22T05:12:23.589094Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:544: UserWarning: The parameter 'ngram_range' will not be used since 'analyzer' is callable'\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n","15 fits failed out of a total of 30.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","15 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n","    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n","  File \"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n","    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n","  File \"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n","    raise ValueError(\n","ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.42912432        nan 0.43136496        nan 0.43002058]\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters:  {'C': 1, 'penalty': 'l2'}\n"]}],"source":["# Filter the train data\n","x_train = x_train.apply(lambda x: preprocessText(x))\n","  \n","# Feature extraction\n","# Define a custom analyzer that applies stemming to the tokens\n","stemmer = ISRIStemmer()\n","analyzer = TfidfVectorizer().build_analyzer()\n","def stemmed_words(doc):\n","    return (stemmer.stem(w) for w in analyzer(doc))\n"," \n","# Create a Vectorizer Object\n","vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=5000, analyzer=stemmed_words) \n"," \n","# Fit and transform the Vectorizer Object on the train data\n","x_train = vectorizer.fit_transform(x_train) \n","y_train = y_train.values # The true labels of the train data\n","\n","# Define the SVM model\n","model = LinearSVC()\n","# Define the hyperparameters to tune\n","hyperparameters = { \n","    'C': [0.1, 1, 10],\n","    'penalty': ['l1', 'l2'] \n","}\n","\n","# Use GridSearchCV to find the best hyperparameters\n","grid = GridSearchCV(model, hyperparameters)\n","grid.fit(x_train, y_train)\n","\n","# Print the best hyperparameters\n","print(\"Best hyperparameters: \", grid.best_params_)\n","\n","# Get the best model\n","best_model = grid.best_estimator_"]},{"cell_type":"markdown","metadata":{},"source":["# Testing"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T05:43:13.527694Z","iopub.status.busy":"2023-05-22T05:43:13.527002Z","iopub.status.idle":"2023-05-22T05:44:46.615772Z","shell.execute_reply":"2023-05-22T05:44:46.614767Z","shell.execute_reply.started":"2023-05-22T05:43:13.527655Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.4327106067219555\n"]}],"source":["# Evaluate the model\n","\n","# Preprocess the test data\n","x_test = x_test.apply(preprocessText)\n","\n","# Feature extraction\n","x_test = vectorizer.transform(x_test) # Encode the data\n","\n","# Predict the labels of the test data using the best model\n","y_test = y_test.values # The true labels of the test data\n","y_predict = grid.predict(x_test)\n","\n","# Calculate the accuracy\n","accuracy = accuracy_score(y_test, y_predict)\n","print(\"Accuracy: \", accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["# Arabic Dialect Identification system"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T08:37:50.538480Z","iopub.status.busy":"2023-05-22T08:37:50.538098Z","iopub.status.idle":"2023-05-22T08:38:01.316332Z","shell.execute_reply":"2023-05-22T08:38:01.315381Z","shell.execute_reply.started":"2023-05-22T08:37:50.538449Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ÙØ¶Ù„Ø§Ù‹ Ø§Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ: Ø§Ø²ÙŠÙƒ ÙŠØ§ Ø¨Ø§Ø´Ø§\n"]},{"name":"stdout","output_type":"stream","text":["Ù„Ù‡Ø¬Ù‡ Ø§Ù„Ù†Øµ Ù‡ÙŠ: Ù…ØµØ±ÙŠ\n"]}],"source":["# Take text from user\n","userText = input(\"ÙØ¶Ù„Ø§Ù‹ Ø§Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ:\")\n","\n","# Convert string into an DataFrame\n","text = [userText]\n","text = pd.DataFrame(text)\n","\n","# Filter the text\n","text[0] = text[0].apply(preprocessText)\n","\n","# Encode the text\n","text_user = vectorizer.transform(text[0]) \n","\n","# The prediction made by the model\n","predict = best_model.predict(text_user)\n","\n","# Display the result\n","print(\"Ù„Ù‡Ø¬Ù‡ Ø§Ù„Ù†Øµ Ù‡ÙŠ:\",predict[0])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
